{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFINED_HDN_3\n",
    "\n",
    "Best result so far is provided by applying a 3D N2V on the video stacks.\n",
    "Some noise is still present, although its variance in the time dimension has been greatly reduced. \n",
    "\n",
    "In this experiment we apply Hierarchical DivNoising to the data to remove more noise.\n",
    "HDN requires to build a Noise Model. This is usually done by training a Gaussian Mixture Model or a simpler histogram model on Low and High SNR pairs.\n",
    "Since we don't have such pairs at our disposal we can approximate the noise model by comparing the output of REFINED_N2V_3 and the raw dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping a Noise Model using REFINED_N2V_3 outputs\n",
    "\n",
    "To _bootstrap_ a Noise Model we use the output of REFINED_N2V_3 as an High SNR signal and the raw input as the noisy input.\n",
    "\n",
    "To train a GMM Noise Model, run the following code in your terminal / HPC:\n",
    "\n",
    "```\n",
    "\n",
    "python -u train_noise_model.py \n",
    "       -e .env_hpc \n",
    "       --level DEBUG \n",
    "       --signal_folder=/your_path_to/calcium_imaging/refined/train  \n",
    "       --denoised_folder=output/REFINED_N2V_3/train \n",
    "       --experiment_name=REFINED_HDN_3 \n",
    "       --random_perc=0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Noise Model\n",
    "\n",
    "Loading the noise model trained in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"REFINED_HDN_3\"\n",
    "\n",
    "# Datasets\n",
    "TRAIN_DATASET_PATH = \"/localscratch/calcium_imaging_dataset/calcium_imaging/refined/train\"\n",
    "VAL_DATASET_PATH = \"/localscratch/calcium_imaging_dataset/calcium_imaging/refined/val\"\n",
    "TEST_DATASET_PATH = \"/localscratch/calcium_imaging_dataset/calcium_imaging/refined/test\"\n",
    "\n",
    "signal_folder = TRAIN_DATASET_PATH\n",
    "denoised_folder = \"output/REFINED_N2V_3/train\"\n",
    "gmm_to_load = f\"models/{experiment_name}/noise_model/GMM.npz\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# Training\n",
    "num_epochs = 500\n",
    "lr = 3e-4\n",
    "steps_per_epoch = 400\n",
    "test_batch_size = 100\n",
    "hdn_model_path = f\"models/{experiment_name}/hdn/\"\n",
    "\n",
    "\n",
    "PATCH_SIZE = 64\n",
    "TILE_SIZE = 32\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_shape = [1009, 1024, 1024]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdn.lib.gaussianMixtureNoiseModel import GaussianMixtureNoiseModel\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "noise_model = GaussianMixtureNoiseModel(    \n",
    "                                    path=Path(gmm_to_load).parent, \n",
    "                                    device = device, \n",
    "                                    params = np.load(gmm_to_load, allow_pickle=True)\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute min and max values for setting the visualization range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tifffile\n",
    "# Get vmin, vmax\n",
    "RECOMPUTE = False\n",
    "\n",
    "if RECOMPUTE:\n",
    "    for p in list(Path(signal_folder).rglob(\"*.tif\")):\n",
    "        i = tifffile.imread(p)\n",
    "        s_vmin, s_vmax = i.min(), i.max()\n",
    "    print(f\"Signal min: {s_vmin} max: {s_vmax}\")\n",
    "\n",
    "    for p in list(Path(denoised_folder).rglob(\"*.tif\")):\n",
    "        i = tifffile.imread(p)\n",
    "        o_vmin, o_vmax = i.min(), i.max()\n",
    "    print(f\"Observation min: {o_vmin} max: {o_vmax}\")\n",
    "\n",
    "    \n",
    "else:\n",
    "    # Precomputed value for REFINED dataset\n",
    "    s_vmin, s_vmax = 721, 1032\n",
    "    o_vmin, o_vmax = 783.7182006835938, 999.6793212890625\n",
    "    print(f\"Signal min: {s_vmin} max: {s_vmax}\")\n",
    "    print(f\"Observation min: {o_vmin} max: {o_vmax}\")\n",
    "\n",
    "vmin = math.floor(min(s_vmin, o_vmin))\n",
    "vmax = math.ceil(max(s_vmax, o_vmax))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "def plot_gmm_likelihood(gaussianMixtureNoiseModel, min_signal, max_signal, n_bin, device):\n",
    "    \"\"\"\n",
    "    Interactive function to analyze the GMM likelihood for various signal values.\n",
    "    Uses ipywidgets for dynamic interaction.\n",
    "    \n",
    "    Args:\n",
    "        gaussianMixtureNoiseModel: A GMM with a `likelihood(observations, signal)` method.\n",
    "        min_signal (float): Minimum signal value.\n",
    "        max_signal (float): Maximum signal value.\n",
    "        n_bin (int): Number of bins for discretizing the signal range.\n",
    "        device (torch.device): Device for tensor computations (CPU/GPU).\n",
    "    \"\"\"\n",
    "    def update(signal_value):\n",
    "        # Discretize the observation range\n",
    "        bin_size = (max_signal - min_signal) / n_bin\n",
    "        observation_values = np.arange(min_signal, max_signal, bin_size) + bin_size / 2\n",
    "        observations_torch = torch.from_numpy(observation_values).float().to(device)\n",
    "        \n",
    "        # Convert the signal value to tensor\n",
    "        signal_torch = torch.tensor(signal_value, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Compute likelihood from the GMM\n",
    "        likelihood_torch = gaussianMixtureNoiseModel.likelihood(observations_torch, signal_torch)\n",
    "        likelihood_numpy = likelihood_torch.cpu().detach().numpy()\n",
    "        \n",
    "        # Plot the likelihood\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(observation_values, likelihood_numpy, label=f'GMM Likelihood (s = {signal_value:.2f})', color='red', linewidth=2)\n",
    "        plt.ylim(0, 1)  # Fix y-axis scale\n",
    "        plt.xlabel('Observation (x)')\n",
    "        plt.ylabel('Probability Density')\n",
    "        plt.title(f'Probability Distribution P(x|s) for Signal s = {signal_value:.2f}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    # Create interactive slider for signal value\n",
    "    interact(update, signal_value=FloatSlider(value=(min_signal + max_signal) / 2, \n",
    "                                              min=min_signal, max=max_signal, step=(max_signal - min_signal) / 100))\n",
    "\n",
    "plot_gmm_likelihood(noise_model, vmin, vmax, vmax-vmin, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D visualization\n",
    "By plotting the input signal values on the vertical axis and the likelihood on the horizontal axis we can notice that the noise is dependent on the signal value. In particular, pixel intensities tend to have more variance the higher the signal is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_likelihood_as_image(model=noise_model, vmin=vmin, vmax=vmax):\n",
    "    img = list()\n",
    "\n",
    "    for h in range(vmin, vmax+1):\n",
    "        observation_values = np.arange(vmin, vmax)\n",
    "        observations_torch = torch.from_numpy(observation_values).float().to(device)\n",
    "        \n",
    "        # Convert the signal value to tensor\n",
    "        signal_torch = torch.tensor([h], dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Compute likelihood from the GMM\n",
    "        likelihood_torch = model.likelihood(observations_torch, signal_torch)\n",
    "        likelihood_numpy = likelihood_torch.cpu().detach().numpy()\n",
    "        \n",
    "        img.append(likelihood_numpy)\n",
    "        \n",
    "    img = np.array(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "m = plot_likelihood_as_image()\n",
    "plt.imshow(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training HDN Model\n",
    "\n",
    "Once we have obtained a noise model for our data, we can now train the HDN model.\n",
    "HDN is based on LadderVAE architecture, which requires some statistics of the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training HDN is resource-intensive, we can train and predict using tiling and patching.\n",
    "We will use predtiler to manage 2D tiling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Tuple\n",
    "from predtiler.tile_stitcher import stitch_predictions\n",
    "from predtiler.dataset import get_tiling_dataset, get_tile_manager\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import make_predtiler_dataset\n",
    "\n",
    "PatchedCalciumImagingDataset = make_predtiler_dataset(data_shape=data_shape,\n",
    "                                                      tile_size=TILE_SIZE,\n",
    "                                                      patch_size=PATCH_SIZE)\n",
    "\n",
    "train_dataset = PatchedCalciumImagingDataset(TRAIN_DATASET_PATH, patch_size=PATCH_SIZE)\n",
    "val_dataset  = PatchedCalciumImagingDataset(VAL_DATASET_PATH, patch_size=PATCH_SIZE)\n",
    "test_dataset  = PatchedCalciumImagingDataset(TEST_DATASET_PATH, patch_size=PATCH_SIZE)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define HDN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('hdn')\n",
    "\n",
    "from hdn.models.lvae import LadderVAE\n",
    "from hdn import training as hdn_training\n",
    "\n",
    " # Model-specific\n",
    "num_latents = 6\n",
    "z_dims = [32]*int(num_latents)\n",
    "blocks_per_layer = 5\n",
    "batchnorm = True\n",
    "free_bits = 1.0\n",
    "use_uncond_mode_at=[0,1]\n",
    "\n",
    "\n",
    "hdn_model = LadderVAE(z_dims=z_dims,\n",
    "                    blocks_per_layer=blocks_per_layer,\n",
    "                    data_mean=train_dataset.dataset_mean,\n",
    "                    data_std=train_dataset.dataset_std,\n",
    "                    noiseModel=noise_model,\n",
    "                    device=device,\n",
    "                    batchnorm=batchnorm,\n",
    "                    free_bits=free_bits,\n",
    "                    img_shape=[PATCH_SIZE, PATCH_SIZE],\n",
    "                    use_uncond_mode_at=use_uncond_mode_at).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train HDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdn_model.train() # Model set in training mode\n",
    "\n",
    "hdn_training.train_network(model=hdn_model,\n",
    "                    lr=lr,\n",
    "                    max_epochs=num_epochs,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    directory_path= hdn_model_path,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=train_loader,\n",
    "                    test_loader=train_loader,\n",
    "                    virtual_batch=BATCH_SIZE,\n",
    "                    gaussian_noise_std=None,\n",
    "                    model_name=experiment_name,\n",
    "                    val_loss_patience=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.stack(predictions) # prediction should have shape: (number_of_patches, C, patch_size, patch_size)\n",
    "stitched_pred = stitch_predictions(predictions, train_dataset.tile_manager)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4life_calcium_imaging_denoising",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
